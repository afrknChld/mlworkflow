{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlworkflow tutorial\n",
    "\n",
    "This tutorial is not meant to show every possible use or functionality of the mlworkflow library. It is rather to expose some of them that fit in an interactive context so that people may see if some of the features can be useful to them.\n",
    "\n",
    "Our goal is at the moment simply to see if our work could benefit other people. Feedbacks are of course welcome.\n",
    "\n",
    "## What is mlworkflow?\n",
    "\n",
    "It is a library providing several separate modules to:\n",
    "\n",
    "- Have an interactive feedback from your model being trained\n",
    "- Aggregate data about your models and experiments, accessing them in a convenient manner\n",
    "- Help you to obtain replayable and modifiable experiments\n",
    "- Put notes and comments on them\n",
    "- Make the use of your models more practical\n",
    "\n",
    "That the modules are there do not mean you should use them. Only use them if they fit your way of experimenting. If you have other habits that you feel like would deserve a place in the library, we strongly encourage you to try to implement it in a generic fashion and to submit a pull-request, or to suggest it.\n",
    "\n",
    "This library is developped at UCLouvain, and is mostly an attempt to make people feel that \"tensorboard\" does not answer to everything we may want about keeping tracks of our experiments. It is under MIT license.\n",
    "\n",
    "## What is not currently handled that could frustrate users?\n",
    "\n",
    "The interactive parts of the library (LivePanels, Dashboard) mostly rely on the use of Jupyter notebooks. However, a lot of people do not run their experiments in Jupyter notebooks. (Here, we use them as \"front-ends\" to our models, ...) So you may want to create summaries from Python scripts as well and only run the dashboard in a notebook. This problem will most likely be addressed. For the rest (Dataset, DataCollection, ...) nothing should rely on Jupyter. If it does, this should be fixed.\n",
    "\n",
    "There is not much documentation and not all the features are commented yet, but we hope the tutorial may help you to get started with it, or why not getting you to ask for more.\n",
    "\n",
    "Finally, remember this is best described as a side-project for accelerating our research that we thought may benefit other people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some nice code you may want to have in a file for versioning, ...\n",
    "from mlworkflow.datasets import DictDataset, TransformedDataset\n",
    "from mlworkflow.tf_utils import TFModel\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def graph_definition(additional_layers=[], optimizer=tf.train.RMSPropOptimizer):\n",
    "    \"\"\"Some graph definition, could be anything!\"\"\"\n",
    "    x = inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    \n",
    "    with tf.variable_scope(\"shared\"):\n",
    "        x = tf.layers.conv2d(x, 64, kernel_size=[3,3], strides=[2,2], padding=\"same\", activation=tf.nn.relu)\n",
    "        x = tf.layers.conv2d(x, 128, kernel_size=[3,3], strides=[2,2], padding=\"same\", activation=tf.nn.relu)\n",
    "        x = tf.layers.conv2d(x, 256, kernel_size=[3,3], strides=[2,2], padding=\"same\", activation=tf.nn.relu)\n",
    "    \n",
    "    x = tf.layers.flatten(x)\n",
    "    \n",
    "    for layer_defn in additional_layers:\n",
    "        x = layer_defn(x)\n",
    "    \n",
    "    x = logits = tf.layers.dense(x, units=10)\n",
    "    \n",
    "    y_pred = tf.nn.softmax(x)\n",
    "    y_true = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    training = tf.placeholder(tf.bool)  # Would only be useful for batchnorm or dropout\n",
    "    learning_rate = tf.Variable(1e-3)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=logits)\n",
    "    loss = tf.reduce_sum(loss, axis=0)\n",
    "    \n",
    "    optimizer = optimizer(learning_rate)\n",
    "    step = optimizer.minimize(loss)\n",
    "    \n",
    "    # \"inputs\", \"outputs\", \"targets\", \"loss\", \"step\" and \"training\" are necessary\n",
    "    # for TFModel.train, TFModel.predict and TFModel.evaluate require less,\n",
    "    # but this is meant to be easily modifiable to suit your needs.\n",
    "    return dict(inputs=inputs,\n",
    "                outputs=y_pred,\n",
    "                targets=y_true,\n",
    "                lr=learning_rate,\n",
    "                logits=logits,\n",
    "                loss=loss,\n",
    "                step=step,\n",
    "                training=training)\n",
    "\n",
    "\n",
    "def dataset_definition(return_keys=False):\n",
    "    # We create a mlworkflow.dataset DictDataset, but this is just for my convenience\n",
    "    # and later being able to quickly integrate data augmentation, ...\n",
    "    # There are much more powerful tools than DictDataset... (see PickledDataset,\n",
    "    # pickle_or_load, TransformedDataset, AugmentedDataset...)\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets(\"tutorial_files/MNIST/\", one_hot=True)\n",
    "    keys, dic = {}, {}\n",
    "    \n",
    "    dic.update({i: item\n",
    "                for i, item in enumerate(zip(mnist.train.images,\n",
    "                                             mnist.train.labels))})\n",
    "    keys[\"training_keys\"] = list(range(len(dic)))\n",
    "    \n",
    "    old_size = len(dic)\n",
    "    dic.update({i: item\n",
    "                for i, item in enumerate(zip(mnist.validation.images,\n",
    "                                             mnist.validation.labels),\n",
    "                                         old_size)})\n",
    "    keys[\"validation_keys\"] = list(range(old_size, len(dic)))\n",
    "    \n",
    "    old_size = len(dic)\n",
    "    dic.update({i: item\n",
    "                for i, item in enumerate(zip(mnist.test.images,\n",
    "                                             mnist.test.labels),\n",
    "                                             old_size)})\n",
    "    keys[\"testing_keys\"] = list(range(old_size, len(dic)))\n",
    "    \n",
    "    dataset = DictDataset(dic)\n",
    "    dataset = TransformedDataset(dataset)\n",
    "    @dataset.add_transform\n",
    "    def reshape(item):\n",
    "        x, y = item\n",
    "        x = np.reshape(x, [28, 28, 1])\n",
    "        return x, y\n",
    "    \n",
    "    if return_keys:\n",
    "        return dataset, keys\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting\n",
    "\n",
    "We first create our model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlworkflow import DataCollection, Call\n",
    "# We create a persistent representation of how our model and dataset are instantiated,\n",
    "# and instantiate them, as well as log some other data\n",
    "# Notice all of this is framework-agnostic! (except what comes from mlworkflow.tf_utils)\n",
    "\n",
    "data = DataCollection(\"tutorial_files/experiment_{}.dcp\")\n",
    "data[\"model\"] = Call(TFModel)(initializer=Call(graph_definition)(\n",
    "    # Let's not add a single layer for now\n",
    "    additional_layers=[]  # Empty list by default\n",
    ").partial())\n",
    "print(\"A picklable and replayable representation of the creation of the model:\\n\", data[\"model\"])\n",
    "print(\"Notice it will look for 'graph_definition' in the module it taken it from.\\n\")\n",
    "data[\"dataset\"] = Call(dataset_definition)\n",
    "# You may also want to store the code of some file, whatever, you can put anything\n",
    "# that can be pickled in \"data\"\n",
    "\n",
    "model = data[\"model\"].eval()\n",
    "dataset, keys = data[\"dataset\"](return_keys=True).eval()\n",
    "\n",
    "globals().update(keys)\n",
    "data.update(keys)\n",
    "\n",
    "data.filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlworkflow import run_in_cell\n",
    "from mlworkflow import LivePanels, SideRunner\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import random\n",
    "\n",
    "@run_in_cell  # This will run the body of this function as well as let us reuse it later\n",
    "def train(model, data, epochs=20):\n",
    "    # To run data augmentation in parallel\n",
    "    sd = SideRunner()\n",
    "    # To have progress bar and updateable sub-outputs\n",
    "    panels = LivePanels([\"head\", \"batch_prog\", \"body\", \"val_perf\"], record=[\"body\"])\n",
    "\n",
    "    for epoch in panels.tqdm_notebook(range(epochs)):\n",
    "        data[[\"training_error\", \"validation_error\"]] = 0, 0\n",
    "        # randomize batch composition\n",
    "        random.shuffle(training_keys);random.shuffle(validation_keys)\n",
    "        n = 0\n",
    "        for x, y in sd.yield_async(dataset.batches(training_keys, 2048)):\n",
    "            data[\"training_error\"] += model.train(x, y)\n",
    "            n += x.shape[0]\n",
    "            panels[\"batch_prog\"] = \"Training {}/{}\".format(n, len(training_keys))\n",
    "        data[\"training_error\"] /= n\n",
    "\n",
    "        n = 0\n",
    "        for x, y in sd.yield_async(dataset.batches(validation_keys, 2048)):\n",
    "            data[\"validation_error\"] += model.train(x, y)\n",
    "            n += x.shape[0]\n",
    "            panels[\"batch_prog\"] = \"Validating {}/{}\".format(n, len(validation_keys))\n",
    "        data[\"validation_error\"] /= n\n",
    "\n",
    "        display.clear_output()\n",
    "        # History WITH CURRENT VALUES APPENDED (only because of the \"_\" of \"history_\"\n",
    "        plt.plot(data.history_[:,[\"training_error\", \"validation_error\"]])\n",
    "        plt.show()\n",
    "        # We could also show some examples\n",
    "\n",
    "        data.save_external(\"weights\", model.get_variables())\n",
    "\n",
    "        data[\"recording\"] = panels.current_record\n",
    "        data.checkpoint()\n",
    "    return \"some_return\"\n",
    "assert train.result == \"some_return\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlworkflow import ListFromArgs\n",
    "restore_weights = False\n",
    "new_data, data = DataCollection.create_with_parent(\"tutorial_files/experiment_{}.dcp\",\n",
    "                                                   parent=data.filename)\n",
    "# Restore everything from that experiment, we'll only change the model\n",
    "new_data.update(data[-1])\n",
    "# Just change the optimizer, add some layers\n",
    "new_data[\"model\"] = data[-1,\"model\"](\n",
    "    initializer=data[-1,\"model\"][\"initializer\"](\n",
    "        optimizer=Call(\"tensorflow\", \"train.AdamOptimizer\").partial(),  # Some hand-made reference\n",
    "        # Creates a list with children that have to be evaluated\n",
    "        additional_layers=Call(ListFromArgs).with_args(  \n",
    "            Call(tf.layers.dense)(units=100, activation=tf.nn.relu).partial()\n",
    "        )\n",
    "    )\n",
    ")\n",
    "print(new_data[\"model\"])\n",
    "print(\"Please note that the activation is picklable (because accessible globally), but does not have \"\n",
    "      \"a reproducible string. We could wrap it into Call(...).partial()\")\n",
    "new_model = new_data[\"model\"].eval()\n",
    "# We could even restore the weights from the common parts\n",
    "if restore_weights:\n",
    "    new_model.set_variables({k:v for k, v in data.external[-1,\"weights\"]\n",
    "                             if k.startswith(\"shared/\")})\n",
    "# Reuse the \"train\" function from\n",
    "train(new_model, new_data, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare both experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlworkflow import find_files\n",
    "# Take the last two files\n",
    "runs = DataCollection.load_files(find_files(\"tutorial_files/*.dcp\")[-2:])\n",
    "# Compare last 10 training errors\n",
    "print(\"Training error\")\n",
    "plt.plot(np.array(runs[:,-10:,\"training_error\"]).T)\n",
    "plt.show()\n",
    "# plot validation_error\n",
    "print(\"Validation error\")\n",
    "for run in runs.values():\n",
    "    plt.plot(run[:,\"validation_error\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mlworkflow import dashboard as d\n",
    "d.Dashboard([[d.list_files(path=\"tutorial_files/*.dcp\", reverse=True),\n",
    "              d.vbox(d.tags(), d.comment())  # Tags and comments in metadata\n",
    "             ],\n",
    "             [d.filename(link_parent=True)],  # Show filename and link to parent if any\n",
    "             [d.list_properties()],  # List fields of data and meta_data\n",
    "             [d.recording()],  # Replay outputs recorded by panels into data[\"recording\"]\n",
    "             [d.execute(globals())],  # Plays some code in ```code sections``` in the comment metadata\n",
    "            ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
